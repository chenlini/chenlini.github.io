<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Algorithm | Chenlini]]></title>
  <link href="http://chenlini.github.io/blog/categories/algorithm/atom.xml" rel="self"/>
  <link href="http://chenlini.github.io/"/>
  <updated>2017-06-20T16:27:51+08:00</updated>
  <id>http://chenlini.github.io/</id>
  <author>
    <name><![CDATA[Chenlini]]></name>
    <email><![CDATA[chenlini1234@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Softmax]]></title>
    <link href="http://chenlini.github.io/blog/2017/06/16/softmax/"/>
    <updated>2017-06-16T17:52:28+08:00</updated>
    <id>http://chenlini.github.io/blog/2017/06/16/softmax</id>
    <content type="html"><![CDATA[<h1>SoftMax</h1>

<h3>原理：</h3>

<p>LR实际上是针对二分类问题的，但是SoftMax其实就是LR推广到多分类的形式。
在逻辑回归中：</p>

<p>$P(y=1|x)=\frac{e^{-\theta<sup>Tx</sup>}}{1+e^{-\theta<sup>Tx</sup>}}$
$P(y=0|X)=\frac{1}{1+e^{-\theta<sup>Tx</sup>}}$</p>

<p>那么loss function就是：</p>

<p>$L(\theta)=\sum_{i=1}^n{y<sup>i</sup>}log\frac{e^{-\theta<sup>Tx</sup>}}{1+e^{-\theta<sup>Tx</sup>}}+(1-y<sup>i</sup>)log\frac{1}{1+e^{-\theta<sup>Tx</sup>}}$</p>

<p>推广到多分类问题：</p>

<p>$P(y=k|x)=\frac{e^{-\theta_k<sup>Tx</sup>}}{\sum_k<sup>Ke</sup>^{-\theta_k<sup>Tx</sup>}}$</p>

<p>那么loss function就变成了：</p>

<p>$L(\theta)=\sum_{i=1}^n{I{y<sup>i</sup>=k}}log\frac{e^{-\theta_k<sup>Tx</sup>}}{1+e^{-\theta_k<sup>Tx</sup>}}$</p>

<p>$I{y<sup>i</sup> = k}$表示当y<sup>i</sup> = k时为1，其他为0.</p>

<p>同样可以采用梯度下降的方式求解。</p>

<h3>实现demo</h3>

<p>SKlean中关于逻辑回归的多分类问题，可以选择muti参数，它是有多个二分类器组合实现的。SKlean没有实现softmax的回归分类算法。在Theano包中有SoftMax函数，用与计算公式3中的值，可以自己实现基于SoftMax的分类算法。</p>

<pre><code># encoding:utf-8
"""
@version:1
@author:Chenlini
@file:softmax.py
@time:2017/6/15 11:22
"""

# coding:utf8


from theano import *
from sklearn.datasets import *
from pandas import *
from sklearn.model_selection import *
from numpy import *


class SoftMax:
    def __init__(self, Iter=1, learnrate=0.01, alpha=0.1):

        self.Iter = Iter
        self.learnrate = learnrate
        self.alpha = alpha

    def training(self, X, Y, typenum, batch_size=500):
        ##featuresnum是X的纬度，batchesnum是分区的数目
        featuresnum = X.shape[1]
        batchesnum = int(X.shape[0] / batch_size)
        ###因为在softMax中theta和x都是全局变量，所以这里应该声明为全局变量
        X = theano.shared(np.asarray(X, dtype=float))
        ###保证Y是整型的
        Y = theano.tensor.cast(theano.shared(np.asarray(Y)), 'int32')
        #x是一个矩阵，y是一个向量
        x = theano.tensor.matrix('x')
        y = theano.tensor.ivector('y')
        ##表示index是整型标量
        index = theano.tensor.lscalar()
        theta = theano.shared(value=0.001 * zeros((featuresnum, typenum),
                                                     dtype=theano.config.floatX),
                              name='theta', borrow=True)
        ###hx就是计算softmax()的值
        hx = theano.tensor.nnet.softmax(theano.tensor.dot(x, theta))
        ##这个就是loss
        cost = -theano.tensor.mean(theano.tensor.log(hx)[theano.tensor.arange(y.shape[0]), y]) + 0.5 * self.alpha * theano.tensor.sum(theta ** 2)
        #grad可以自动计算倒数
        g_theta = theano.tensor.grad(cost, theta)
        ##这里就是用一个结构保存更新的值
        updates = [(theta, theta - self.learnrate * g_theta)]
        #批量更新
        train_model = theano.function(
            inputs=[index], outputs=cost, updates=updates, givens={
                x: X[index * batch_size: (index + 1) * batch_size],
                y: Y[index * batch_size: (index + 1) * batch_size]
            }, allow_input_downcast=True
        )

        lastcostJ = np.inf
        stop = False
        epoch = 0
        costj = []
        ###总体的循环执行，当达到最大循环次数或者cost不再下降的时候结束
        while (epoch &lt; self.Iter) and (not stop):
            epoch = epoch + 1
            for minibatch_index in range(batchesnum):
                costj.append(train_model(minibatch_index))
            if np.mean(costj) &gt;= lastcostJ:
                print("costJ is increasing !!!")
                stop = True
            else:
                lastcostJ = np.mean(costj)
                print(('epoch %i, minibatch %i/%i,averange cost is %f') %
                      (epoch, minibatch_index + 1, batchesnum, lastcostJ))
        self.theta = theta
        return self.theta.get_value()



    def testing(self, X, Y, batch_size=500):

        batchesnum = int(X.shape[0] / batch_size)
        X = theano.shared(np.asarray(X, dtype=float))
        Y = theano.tensor.cast(theano.shared(np.asarray(Y)), 'int32')

        x = theano.tensor.matrix('x')
        y = theano.tensor.ivector('y')

        index = theano.tensor.lscalar()
        hx = theano.tensor.nnet.softmax(theano.tensor.dot(x, self.theta))
        ###求最后的类别结果和误差
        predict = theano.tensor.argmax(hx, axis=1)

        errors = theano.tensor.mean(theano.tensor.neq(predict, y))

        test_model = theano.function(
            inputs=[index], outputs=errors, givens={
                x: X[index * batch_size: (index + 1) * batch_size],
                y: Y[index * batch_size: (index + 1) * batch_size]
            }, allow_input_downcast=True
        )
        test_losses = []
        for minibatch_index in range(batchesnum):
            test_losses.append(test_model(minibatch_index))
        test_score = np.mean(test_losses)
        print(('minibatch %i/%i, test error of model %f %%') %
              (minibatch_index + 1, batchesnum, test_score * 100.))




if __name__ == '__main__':

    X, y = make_classification(n_samples=80000)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)
    Testsoftmax = SoftMax()
    ###typenum=len(set(y)
    Testsoftmax.training(X_train, y_train, typenum=len(set(y)))
    Testsoftmax.testing(X_test, y_test)
</code></pre>

<p>基本代码参考：
<a href="http://www.cnblogs.com/qw12/p/5962430.html">http://www.cnblogs.com/qw12/p/5962430.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GBDT+LR]]></title>
    <link href="http://chenlini.github.io/blog/2017/06/13/gbdt-plus-lr/"/>
    <updated>2017-06-13T16:52:56+08:00</updated>
    <id>http://chenlini.github.io/blog/2017/06/13/gbdt-plus-lr</id>
    <content type="html"><![CDATA[<h1>GBDT+LR融合</h1>

<h2>原理</h2>

<p>工业界LR模型用的比较多的原因就是它非常容易可以并行化，处理起大规模的数据来比较容易。但是LR的学习能力有限，必须要经历大量的特征选择，特征组合等工作。因此找到合适的方法简化特征工程是非常重要的。facebook在KDD上发表了一篇文章，讲的就是GBDT+LR，为我们提供了一些思路，论文中的基本原理很简单，搞清楚下面这幅图就行了：</p>

<p><img src="./images/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-06-13%2016.41.39.png" alt="" /></p>

<blockquote><ol>
<li><p>使用GBDT训练数据，得到如下2棵树，第一棵树3个叶子结点，第二棵树2个叶子结点，总共5个叶子结点。因此最后用于LR的特征向量就是5维的[0,0,0,0,0]。</p></li>
<li><p>训练数据共有n个样本，那么对于其中一个样本x,查看x最终会出现在哪些树的结点上面。比如x出现在树1的第1个结点以及树2的第2个结点，那么特征向量就是[1,0,0,0,1]
得到的特征向量可以选择和原本的特征一起带入逻辑回归中，也可以单独作为特征进行学习。</p></li>
</ol>
</blockquote>

<h2>demo</h2>

<p>sklearn分别实现了GBDT和LR的算法，所以做一个小例子测试融合算法是否会比原始的算法好。重点介绍方法，结果不确定，因为参数没有调到最优。</p>

<pre><code>import numpy as np
from sklearn import ensemble
from sklearn import  datasets
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_hastie_10_2
from sklearn.ensemble import GradientBoostingClassifier
</code></pre>

<h6>注意：Boosting的方法也分分类树和回归树，这里选择的是分类树，因为make_hastie_10_2这个数据集是一个二分类的数据集</h6>

<h3>单纯使用GradientBoostingClassifier</h3>

<pre><code>X, y = make_hastie_10_2(random_state=0)
X_train, X_test = X[:2000], X[2000:]
y_train, y_test = y[:2000], y[2000:]
clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,    max_depth=1, random_state=0).fit(X_train, y_train)
sum(clf.predict(X_test)==y_test)
</code></pre>

<p>最后一行是统计分类正确的数目，这里的结果是9130</p>

<h4>单纯使用LogisticRegression</h4>

<pre><code>paramsLr={ 'multi_class':'multinomial',
          'penalty':'l2','solver':'sag', 'tol':0.0001
          }
lr1 = LogisticRegression(C=1.0,solver='sag', max_iter=100,penalty='l2',
                             multi_class='multinomial',tol=0.0001)
lr1.fit(X_train, y_train)
sum(lr1.predict(X_test)==y_test)
</code></pre>

<p>最后一行是统计分类正确的数目，这里的结果是5149</p>

<h4>GBDT+LR融合</h4>

<p>基于原理中介绍到的思想，需要将GBDT运行的结果转化为LR能够接受的数据结构。</p>

<pre><code>def PreLR_GBDT(clf,Features):
    result=clf.apply(Features)[:,:,0]
    classT=np.zeros(result.shape[1]+1)
    for num in range(0,result.shape[1]):
        classT[num+1]=max(result[:,num])

    classT=classT.cumsum()
    classT=classT.astype(np.int32)

    M=np.zeros((result.shape[0],classT[-1]))

    for num in range(0, result.shape[0]):
        for num2 in range(0, result.shape[1]):
            M[num, classT[num2] + result[num][num2] - 1] = 1

    return M
</code></pre>

<p>有了上面的函数之后就可以再建立LR的模型了：</p>

<pre><code>X_train_LR_GBDT=np.append(X_train,PreLR_GBDT(clf, X_train),axis=1)
X_test_LR_GBDT=np.append(X_test,PreLR_GBDT(clf, X_test),axis=1)

lr_GBDT = LogisticRegression(solver='sag', max_iter=100,penalty='l2',
                             multi_class='multinomial',tol=0.0001)
lr_GBDT.fit(X_train_LR_GBDT, y_train)

sum(lr_GBDT.predict(X_test_LR_GBDT)==y_test)
</code></pre>

<p>最后一行是统计分类正确的数目，这里的结果是9140，这里采用的是GBDT训练过后的feature和原始组合的方式，当然和可以不组合，直接用新的feature进行训练。</p>

<p>以上展示的是如何把GBDT与LR融合到一起，每个模型都没有进行调参，因此结果参考意义不大。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[感知机]]></title>
    <link href="http://chenlini.github.io/blog/2017/03/15/gan-zhi-ji/"/>
    <updated>2017-03-15T19:53:19+08:00</updated>
    <id>http://chenlini.github.io/blog/2017/03/15/gan-zhi-ji</id>
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>


<h4>总结：感知机属于二分类的算法，输入为一系列的特征向量，输出为＋1或者是－1.输入空间到输出空间用的是sign()函数。</h4>

<h3>感知机定义了一个超平面：WX＋B＝0</h3>

<p>其中W为权重向量，X为输入的特征向量，B为误差常数。</p>

<p>对于感知机算法来说，训练数据集如果能够被一个超平面完美的划分成正实例和负实例，则称这个数据集为线性可分的，否则就是线性不可分的。</p>

<!--more-->


<ol>
<li><p>损失函数的定义：</p>

<p> 那么如何评价这个感知机算法构造的分割平面是不是好的呢？最直观的方法就是统计该误分点的个数，但是以此来作为损失函数的话，对于W和B来说并不是连续可导的，因此可以将误分点到分割平面的距离之和作为损失函数来进行优化。</p>

<p> （1）误分点的定义：
     实际上，点到分割平面的距离就是｜WX+B｜除以W的F范数（可以不用考虑）。如果是误分点的话，－y(WX+B)肯定是大于0的，以此代替绝对值符号。</p>

<p> 损失函数最后的形式就是：</p>

<p> $$L(w,b)=\sum_{x_i \in M}y_i(w \bullet x_i+b)$$</p>

<p> 求解方法就是求导之后用梯度下降的方法解决。</p></li>
<li><p>感知机除了这个原始形式以外还有一种对偶的形式。主要的思想是W和B可以表示为实例（x,y）的线性组合。</p></li>
</ol>


<p>注意：</p>

<ol>
<li>感知机是线性模型，不能表示复杂的函数</li>
<li>样本线性可分的充要条件是正向实例构成的凸壳与负向实例构成的凸壳不相交。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深入浅出SVM]]></title>
    <link href="http://chenlini.github.io/blog/2017/03/15/shen-ru-qian-chu-svm/"/>
    <updated>2017-03-15T15:14:26+08:00</updated>
    <id>http://chenlini.github.io/blog/2017/03/15/shen-ru-qian-chu-svm</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
</feed>
